{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3IeAtH_gNGO",
    "outputId": "705ae1ce-b2fa-480d-ec8d-8d6cf3b6ee13"
   },
   "outputs": [],
   "source": [
    "# #@ Installing dependencies\n",
    "# !pip install --user langchain\n",
    "# !pip install --user wikipedia\n",
    "# !pip install --user openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  wikipedia\n",
    "# %pip install --user tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0eSx3wlgQ3R"
   },
   "source": [
    "# Getting Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k1eMC7pAgnSq"
   },
   "outputs": [],
   "source": [
    "#@ Loading data from openai wikipedia page\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "data = WikipediaLoader(query=\"openai\").load()\n",
    "\n",
    "## Creating Chunks\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=20)                     # character splitter\n",
    "splits = splitter.split_documents(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QniU548Fh2kD"
   },
   "outputs": [],
   "source": [
    "#@ deleting summary data\n",
    "for split in splits:\n",
    "  del split.metadata[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0hTzhMJiTHM",
    "outputId": "f7aa04f3-ea41-433b-daba-fa83bcaeb849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/OpenAI\n",
      "https://en.wikipedia.org/wiki/OpenAI_Codex\n",
      "https://en.wikipedia.org/wiki/ChatGPT\n",
      "https://en.wikipedia.org/wiki/GPT-3\n",
      "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\n",
      "https://en.wikipedia.org/wiki/Greg_Brockman\n",
      "https://en.wikipedia.org/wiki/Ilya_Sutskever\n",
      "https://en.wikipedia.org/wiki/Sam_Altman\n",
      "https://en.wikipedia.org/wiki/GPT-4\n",
      "https://en.wikipedia.org/wiki/DALL-E\n",
      "https://en.wikipedia.org/wiki/OpenAI_Five\n",
      "https://en.wikipedia.org/wiki/Mira_Murati\n",
      "https://en.wikipedia.org/wiki/GitHub_Copilot\n",
      "https://en.wikipedia.org/wiki/Emmett_Shear\n",
      "https://en.wikipedia.org/wiki/Removal_of_Sam_Altman_from_OpenAI\n",
      "https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)\n",
      "https://en.wikipedia.org/wiki/Gemini_(language_model)\n",
      "https://en.wikipedia.org/wiki/GPT-2\n",
      "https://en.wikipedia.org/wiki/Auto-GPT\n",
      "https://en.wikipedia.org/wiki/Helen_Toner\n",
      "https://en.wikipedia.org/wiki/Bard_(chatbot)\n",
      "https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback\n",
      "https://en.wikipedia.org/wiki/Andrej_Karpathy\n",
      "https://en.wikipedia.org/wiki/Anthropic\n",
      "https://en.wikipedia.org/wiki/Worldcoin\n"
     ]
    }
   ],
   "source": [
    "#@ Inspecting source\n",
    "for split in splits:\n",
    "  print(split.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d5mJ19Bniusr"
   },
   "outputs": [],
   "source": [
    "#@ removing irrelevant sources\n",
    "splits.remove(splits[1])\n",
    "splits.remove(splits[11])\n",
    "splits.remove(splits[12])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HL-6lSHjc3J"
   },
   "source": [
    "# Generating summaries of data and relationship extraction form database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "euIBPZv6kCt_",
    "outputId": "39a63b5d-6b78-4a26-e57b-ee75ede37134"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\.conda\\envs\\bookqa\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\ASUS\\.conda\\envs\\bookqa\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "# Initialize the text splitter\n",
    "rtext_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key=\"\")                            # enter your api key here.\n",
    "\n",
    "# Define the map prompt template\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{all_data}\n",
    "Based on this list of docs, please perform concise summaries while extracting essential relationships for relationships analysis later, please do include dates of actions or events, which are very important for timeline analysis later. Example: \"Sam gets fired by the OpenAI board on 11/17/2023 or (Nov. 17th, Friday)\", which showcases not only the relationship between Sam and OpenAI, but also when it happens.\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# Define the map_chain\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "all_data = splits\n",
    "\n",
    "# Extract text from each document\n",
    "all_text_data = [split.page_content for split in splits]\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{all_data}\n",
    "Take these and distill it into concise summaries of the articles while containing important relationships and events (including the timeline). Example: \"Sam gets fired by the OpenAI board on 11/17/2023 or (Nov. 17th, Friday)\", which showcases not only the relationship between Sam and OpenAI, but also when it happens.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "# ChatPromptTemplate(input_variables=['all_data'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['all_data'], template='The following is a set of documents:\\n{all_data}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])\n",
    "\n",
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain,\n",
    "    document_variable_name=\"all_data\"  # This should match the variable name in reduce_prompt\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"all_data\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(all_data)\n",
    "\n",
    "# Run the MapReduce Chain\n",
    "summarization_results = map_reduce_chain.run(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78462\n"
     ]
    }
   ],
   "source": [
    "length = 0\n",
    "for text in all_text_data:\n",
    "    length += len(text)\n",
    "print(length)                                               # length before summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2397"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summarization_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. OpenAI, a U.S.-based AI research organization, was founded in December 2015 with the goal of developing safe and beneficial artificial general intelligence. It consists of the non-profit OpenAI, Inc. and its for-profit subsidiary OpenAI Global, LLC.\\n2. Microsoft made significant investments in OpenAI, providing $1 billion in 2019 and $10 billion in 2023, including compute resources on Microsoft's Azure cloud service.\\n3. On November 17, 2023, Sam Altman was fired by the OpenAI board, but later returned as CEO. Greg Brockman was also removed as chairman and resigned as president, but returned to the company. Most of the board members resigned, and Bret Taylor became the new chairman.\\n4. OpenAI developed GPT-3, a large language model released in 2020 with 175 billion parameters. GPT-4 was released in March 2023.\\n5. OpenAI launched ChatGPT, a chatbot, on November 30, 2022, and gained over 100 million users by January 2023. It contributed to OpenAI's valuation of $29 billion.\\n6. OpenAI operates ChatGPT as a freemium model, with a free tier accessing the GPT-3.5-based version and paid subscribers getting access to the GPT-4-based version and newer features.\\n7. Concerns have been raised about ChatGPT's potential to displace human intelligence, enable plagiarism, and fuel misinformation.\\n8. OpenAI has developed text-to-image models called DALL·E, DALL·E 2, and DALL·E 3.\\n9. Mira Murati is the Chief Technology Officer of OpenAI since 2018 and briefly served as the interim CEO in November 2023.\\n10. Emmett Shear, former CEO of Twitch, served as the interim CEO of OpenAI in November 2023 but was later replaced by Sam Altman.\\n11. Whisper is an open-source speech recognition and transcription model developed by OpenAI. It was released in September 2022 and has been updated to Whisper V2 and Whisper V3.\\n12. Gemini is a multimodal language model developed by Google DeepMind, announced in December 2023 as a competitor to OpenAI's GPT-4.\\n13. GPT-2 is a large language model developed by OpenAI, released in November 2019 with a generative pre-trained transformer architecture.\\n14. Auto-GPT is an open-source AI agent developed by Toran Bruce Richards, utilizing OpenAI's GPT-4 or GPT-3.5 APIs to automate tasks in various domains.\\n15. Helen Toner, an Australian researcher, served as a board member of OpenAI until November 2023 and published a report on artificial intelligence.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_results                                      # length after sumamrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Storing sumamrization results\n",
    "with open('summary.txt', 'w') as file:\n",
    "    file.write(str(summarization_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting entities and relationships for knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Integrates Large Language Models (LLMs) into spaCy pipelines, featuring a modular system for fast prototyping and prompting\n",
    "# !pip install spacy-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli import download\n",
    "download('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 1[ENT0:CARDINAL]. OpenAI, a U.S.-based AI[ENT1:ORG] research organization,\n",
      "was founded in December 2015[ENT2:DATE] with the goal of developing safe and\n",
      "beneficial artificial general intelligence.\n",
      "Entities: [('1', 'CARDINAL'), ('AI', 'ORG'), ('December 2015', 'DATE')]\n",
      "Relations:\n",
      "  - 1 [ORG] AI\n",
      "  - 1 [DATE] December 2015\n",
      "Text: It consists of the non-profit OpenAI, Inc.[ENT0:ORG] and its for-profit\n",
      "subsidiary OpenAI Global, LLC[ENT1:ORG].\n",
      "Entities: [('OpenAI, Inc.', 'ORG'), ('OpenAI Global, LLC', 'ORG')]\n",
      "Relations:\n",
      "  - OpenAI, Inc. [subsidiary of] OpenAI Global, LLC\n",
      "Text: 2[ENT0:CARDINAL].\n",
      "Entities: [('2', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: Microsoft[ENT0:ORG] made significant investments in OpenAI[ENT1:ORG],\n",
      "providing $1 billion[ENT2:MONEY] in 2019[ENT3:DATE] and $10 billion[ENT4:MONEY]\n",
      "in 2023[ENT5:DATE], including compute resources on Microsoft[ENT6:ORG]'s\n",
      "Azure[ENT7:ORG] cloud service.\n",
      "Entities: [('Microsoft', 'ORG'), ('OpenAI', 'ORG'), ('$1 billion', 'MONEY'),\n",
      "('2019', 'DATE'), ('$10 billion', 'MONEY'), ('2023', 'DATE'), ('Microsoft',\n",
      "'ORG'), ('Azure', 'ORG')]\n",
      "Relations:\n",
      "  - Microsoft [investment] OpenAI\n",
      "  - Microsoft [resource] Microsoft\n",
      "Text: 3[ENT0:CARDINAL].\n",
      "Entities: [('3', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: On November 17, 2023[ENT0:DATE], Sam Altman[ENT1:PERSON] was fired by the\n",
      "OpenAI[ENT2:ORG] board, but later returned as CEO.\n",
      "Entities: [('November 17, 2023', 'DATE'), ('Sam Altman', 'PERSON'), ('OpenAI',\n",
      "'ORG')]\n",
      "Relations:\n",
      "  - Sam Altman [EMPLOYEE] OpenAI\n",
      "Text: Greg Brockman[ENT0:PERSON] was also removed as chairman and resigned as\n",
      "president, but returned to the company.\n",
      "Entities: [('Greg Brockman', 'PERSON')]\n",
      "Relations:\n",
      "  - Greg Brockman [removed_as_chairman] Greg Brockman\n",
      "  - Greg Brockman [resigned_as_president] Greg Brockman\n",
      "Text: Most of the board members resigned, and Bret Taylor[ENT0:PERSON] became\n",
      "the new chairman.\n",
      "Entities: [('Bret Taylor', 'PERSON')]\n",
      "Relations:\n",
      "  - Bret Taylor [new chairman] Bret Taylor\n",
      "Text: 4[ENT0:CARDINAL].\n",
      "Entities: [('4', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: OpenAI developed GPT-3, a large language model released in 2020[ENT0:DATE]\n",
      "with 175 billion[ENT1:MONEY] parameters.\n",
      "Entities: [('2020', 'DATE'), ('175 billion', 'MONEY')]\n",
      "Relations:\n",
      "  - 2020 [AMOUNT] 175 billion\n",
      "Text: GPT-4 was released in March 2023[ENT0:DATE].\n",
      "Entities: [('March 2023', 'DATE')]\n",
      "Relations:\n",
      "Text: 5[ENT0:CARDINAL].\n",
      "Entities: [('5', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: OpenAI launched ChatGPT[ENT0:ORG], a chatbot, on November 30,\n",
      "2022[ENT1:DATE], and gained over 100 million[ENT2:CARDINAL] users by January\n",
      "2023[ENT3:DATE].\n",
      "Entities: [('ChatGPT', 'ORG'), ('November 30, 2022', 'DATE'), ('over 100\n",
      "million', 'CARDINAL'), ('January 2023', 'DATE')]\n",
      "Relations:\n",
      "  - ChatGPT [launch_date] November 30, 2022\n",
      "  - November 30, 2022 [user_count] over 100 million\n",
      "  - January 2023 [user_count] over 100 million\n",
      "Text: It contributed to OpenAI[ENT0:ORG]'s valuation of $29 billion[ENT1:MONEY].\n",
      "Entities: [('OpenAI', 'ORG'), ('$29 billion', 'MONEY')]\n",
      "Relations:\n",
      "  - OpenAI [contributed to valuation] $29 billion\n",
      "Text: 6[ENT0:CARDINAL].\n",
      "Entities: [('6', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: OpenAI operates ChatGPT as a freemium model, with a free tier accessing\n",
      "the GPT-3.5-based version and paid subscribers getting access to the GPT-4-based\n",
      "version and newer features.\n",
      "Entities: []\n",
      "Relations:\n",
      "Text: 7[ENT0:CARDINAL].\n",
      "Entities: [('7', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: Concerns have been raised about ChatGPT[ENT0:ORG]'s potential to displace\n",
      "human intelligence, enable plagiarism, and fuel misinformation.\n",
      "Entities: [('ChatGPT', 'ORG')]\n",
      "Relations:\n",
      "  - ChatGPT [displace human intelligence] ChatGPT\n",
      "  - ChatGPT [enable plagiarism] ChatGPT\n",
      "  - ChatGPT [fuel misinformation] ChatGPT\n",
      "Text: 8[ENT0:CARDINAL].\n",
      "Entities: [('8', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: OpenAI has developed text-to-image models called DALL·E, DALL·E\n",
      "2[ENT0:CARDINAL], and DALL·E 3[ENT1:CARDINAL]. 9[ENT2:CARDINAL].\n",
      "Entities: [('2', 'CARDINAL'), ('3', 'CARDINAL'), ('9', 'CARDINAL')]\n",
      "Relations:\n",
      "  - 2 [HAS_CARDINAL_NUMBER] 9\n",
      "  - 3 [HAS_CARDINAL_NUMBER] 9\n",
      "Text: Mira Murati[ENT0:PERSON] is the Chief Technology Officer of OpenAI since\n",
      "2018[ENT1:DATE] and briefly served as the interim CEO in November\n",
      "2023[ENT2:DATE].\n",
      "Entities: [('Mira Murati', 'PERSON'), ('2018', 'DATE'), ('November 2023',\n",
      "'DATE')]\n",
      "Relations:\n",
      "  - Mira Murati [is the Chief Technology Officer of] 2018\n",
      "Text: 10[ENT0:CARDINAL].\n",
      "Entities: [('10', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: Emmett Shear[ENT0:PERSON], former CEO of Twitch[ENT1:ORG], served as the\n",
      "interim CEO of OpenAI[ENT2:ORG] in November 2023[ENT3:DATE] but was later\n",
      "replaced by Sam Altman[ENT4:PERSON]. 11[ENT5:CARDINAL].\n",
      "Entities: [('Emmett Shear', 'PERSON'), ('Twitch', 'ORG'), ('OpenAI', 'ORG'),\n",
      "('November 2023', 'DATE'), ('Sam Altman', 'PERSON'), ('11', 'CARDINAL')]\n",
      "Relations:\n",
      "  - Emmett Shear [former CEO of] Twitch\n",
      "  - Emmett Shear [interim CEO of] OpenAI\n",
      "  - OpenAI [in November 2023] November 2023\n",
      "  - OpenAI [replaced by] Sam Altman\n",
      "Text: Whisper is an open-source speech recognition and transcription model\n",
      "developed by OpenAI.\n",
      "Entities: []\n",
      "Relations:\n",
      "Text: It was released in September 2022[ENT0:DATE] and has been updated to\n",
      "Whisper V2[ENT1:PERSON] and Whisper V3[ENT2:PRODUCT].\n",
      "Entities: [('September 2022', 'DATE'), ('Whisper V2', 'PERSON'), ('Whisper V3',\n",
      "'PRODUCT')]\n",
      "Relations:\n",
      "  - Whisper V2 [RELEASE] September 2022\n",
      "Text: 12[ENT0:CARDINAL].\n",
      "Entities: [('12', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: Gemini[ENT0:ORG] is a multimodal language model developed by Google\n",
      "DeepMind[ENT1:ORG], announced in December 2023[ENT2:DATE] as a competitor to\n",
      "OpenAI[ENT3:ORG]'s GPT-4. 13[ENT4:CARDINAL].\n",
      "Entities: [('Gemini', 'ORG'), ('Google DeepMind', 'ORG'), ('December 2023',\n",
      "'DATE'), ('OpenAI', 'ORG'), ('13', 'CARDINAL')]\n",
      "Relations:\n",
      "  - Gemini [DEVELOPER] Google DeepMind\n",
      "  - Google DeepMind [ANNOUNCEMENT DATE] December 2023\n",
      "  - Gemini [COMPETITOR] OpenAI\n",
      "Text: GPT-2 is a large language model developed by OpenAI[ENT0:ORG], released in\n",
      "November 2019[ENT1:DATE] with a generative pre-trained transformer architecture.\n",
      "Entities: [('OpenAI', 'ORG'), ('November 2019', 'DATE')]\n",
      "Relations:\n",
      "  - OpenAI [RELEASED_ON] November 2019\n",
      "Text: 14[ENT0:CARDINAL].\n",
      "Entities: [('14', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: Auto-GPT is an open-source AI[ENT0:ORG] agent developed by Toran Bruce\n",
      "Richards[ENT1:PERSON], utilizing OpenAI[ENT2:ORG]'s GPT-4 or GPT-3.5 APIs to\n",
      "automate tasks in various domains.\n",
      "Entities: [('AI', 'ORG'), ('Bruce Richards', 'PERSON'), ('OpenAI', 'ORG')]\n",
      "Relations:\n",
      "  - AI [developed by] Bruce Richards\n",
      "  - AI [utilizing] OpenAI\n",
      "Text: 15[ENT0:CARDINAL].\n",
      "Entities: [('15', 'CARDINAL')]\n",
      "Relations:\n",
      "Text: Helen Toner[ENT0:PERSON], an Australian[ENT1:NORP] researcher, served as a\n",
      "board member of OpenAI[ENT2:ORG] until November 2023[ENT3:DATE] and published a\n",
      "report on artificial intelligence.\n",
      "Entities: [('Helen Toner', 'PERSON'), ('Australian', 'NORP'), ('OpenAI', 'ORG'),\n",
      "('November 2023', 'DATE')]\n",
      "Relations:\n",
      "  - Helen Toner [nationality] Australian\n",
      "  - Helen Toner [affiliation] OpenAI\n",
      "  - Helen Toner [employment.until] November 2023\n",
      "Entity counts: Counter({'ORG': 20, 'CARDINAL': 18, 'DATE': 15, 'PERSON': 9,\n",
      "'MONEY': 4, 'PRODUCT': 1, 'NORP': 1})\n",
      "Relation counts: Counter({'user_count': 2, 'HAS_CARDINAL_NUMBER': 2, 'ORG': 1,\n",
      "'DATE': 1, 'subsidiary of': 1, 'investment': 1, 'resource': 1, 'EMPLOYEE': 1,\n",
      "'removed_as_chairman': 1, 'resigned_as_president': 1, 'new chairman': 1,\n",
      "'AMOUNT': 1, 'launch_date': 1, 'contributed to valuation': 1, 'displace human\n",
      "intelligence': 1, 'enable plagiarism': 1, 'fuel misinformation': 1, 'is the\n",
      "Chief Technology Officer of': 1, 'former CEO of': 1, 'interim CEO of': 1, 'in\n",
      "November 2023': 1, 'replaced by': 1, 'RELEASE': 1, 'DEVELOPER': 1, 'ANNOUNCEMENT\n",
      "DATE': 1, 'COMPETITOR': 1, 'RELEASED_ON': 1, 'developed by': 1, 'utilizing': 1,\n",
      "'nationality': 1, 'affiliation': 1, 'employment.until': 1})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from wasabi import msg\n",
    "from spacy_llm.util import assemble\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"                                           # insert your api key here\n",
    "# traditional spacy NER (Named Recognition Library)\n",
    "def split_document_sent(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents] # referencial\n",
    "\n",
    "# spacy-llm relationship extraction\n",
    "def process_text(nlp, text, verbose=False):\n",
    "    doc = nlp(text)\n",
    "    if verbose:\n",
    "        msg.text(f\"Text: {doc.text}\")\n",
    "        msg.text(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "        msg.text(\"Relations:\")\n",
    "        for r in doc._.rel:\n",
    "            msg.text(f\"  - {doc.ents[r.dep]} [{r.relation}] {doc.ents[r.dest]}\")\n",
    "    return doc\n",
    "\n",
    "def run_pipeline(config_path, examples_path=None, verbose=False):\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        msg.fail(\"OPENAI_API_KEY env variable was not found. Set it and try again.\", exits=1)\n",
    "\n",
    "    nlp = assemble(config_path, overrides={} )\n",
    "\n",
    "    # Initialize counters and storage\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "    relation_counts = Counter()\n",
    "\n",
    "    # Load your articles and news data here\n",
    "    # all_data = news_articles_data + documents\n",
    "\n",
    "    sents = split_document_sent(summarization_results)\n",
    "    for sent in sents:\n",
    "        doc = process_text(nlp, sent, verbose)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        relations = [(doc.ents[r.dep].text, r.relation, doc.ents[r.dest].text) for r in doc._.rel]\n",
    "        \n",
    "        # Store processed data\n",
    "        processed_data.append({'text': doc.text, 'entities': entities, 'relations': relations})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "        relation_counts.update([rel[1] for rel in relations])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open('processed_data.json', 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n",
    "    # Display summary\n",
    "    msg.text(f\"Entity counts: {entity_counts}\")\n",
    "    msg.text(f\"Relation counts: {relation_counts}\")\n",
    "\n",
    "# Set your configuration paths and flags\n",
    "config_path = Path(\"zeroshot.cfg\")\n",
    "examples_path = None  # or None if not using few-shot\n",
    "verbose = True\n",
    "\n",
    "# Run the pipeline\n",
    "file = run_pipeline(config_path, None, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bookqa",
   "language": "python",
   "name": "bookqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
